{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import queue\n",
    "import layers\n",
    "\n",
    "from collections import OrderedDict\n",
    "import ujson as json\n",
    "from json import dumps\n",
    "from ujson import load as json_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "setting up GPUs/CPUs and configuring batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_ids = []\n",
    "if torch.cuda.is_available():\n",
    "    gpu_ids += [gpu_id for gpu_id in range(torch.cuda.device_count())]\n",
    "    device = torch.device('cuda:{}'.format(gpu_ids[0]))\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if len(gpu_ids):\n",
    "#    batch_size = len(gpu_ids) * 64\n",
    "#else:\n",
    "#    batch_size = 1\n",
    "\n",
    "batch_size = max(1, len(gpu_ids)*64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "setting a random seed for consistent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 3716 # 1998 in Hex\n",
    "\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed_all(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "loading in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_temp/word_emb.json\", 'r') as fh: \n",
    "      word_vec_array = np.array(json.load(fh))\n",
    "\n",
    "word_vectors = torch.from_numpy(word_vec_array).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0828,  0.6720, -0.1499,  ..., -0.1918, -0.3785, -0.0659],\n",
      "        ...,\n",
      "        [ 0.3990, -0.0185,  0.3715,  ...,  0.0370, -0.0975, -0.6882],\n",
      "        [-0.0652, -0.0192, -0.8155,  ...,  0.9371, -0.4975, -1.1756],\n",
      "        [ 0.5272, -1.0988, -0.3210,  ..., -0.0951, -0.1996, -1.2129]])\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDAF(nn.Module):\n",
    "    \"\"\"Baseline BiDAF model for SQuAD.\n",
    "\n",
    "    Based on the paper:\n",
    "    \"Bidirectional Attention Flow for Machine Comprehension\"\n",
    "    by Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "    (https://arxiv.org/abs/1611.01603).\n",
    "\n",
    "    Follows a high-level structure commonly found in SQuAD models:\n",
    "        - Embedding layer: Embed word indices to get word vectors.\n",
    "        - Encoder layer: Encode the embedded sequence.\n",
    "        - Attention layer: Apply an attention mechanism to the encoded sequence.\n",
    "        - Model encoder layer: Encode the sequence again.\n",
    "        - Output layer: Simple layer (e.g., fc + softmax) to get final outputs.\n",
    "\n",
    "    Args:\n",
    "        word_vectors (torch.Tensor): Pre-trained word vectors.\n",
    "        hidden_size (int): Number of features in the hidden state at each layer.\n",
    "        drop_prob (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_vectors, hidden_size, drop_prob=0.):\n",
    "        super(BiDAF, self).__init__()\n",
    "        self.emb = layers.Embedding(word_vectors=word_vectors,\n",
    "                                    hidden_size=hidden_size,\n",
    "                                    drop_prob=drop_prob)\n",
    "\n",
    "        self.enc = layers.RNNEncoder(input_size=hidden_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     num_layers=1,\n",
    "                                     drop_prob=drop_prob)\n",
    "\n",
    "        self.att = layers.BiDAFAttention(hidden_size=2 * hidden_size,\n",
    "                                         drop_prob=drop_prob)\n",
    "\n",
    "        self.mod = layers.RNNEncoder(input_size=8 * hidden_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     num_layers=2,\n",
    "                                     drop_prob=drop_prob)\n",
    "\n",
    "        self.out = layers.BiDAFOutput(hidden_size=hidden_size,\n",
    "                                      drop_prob=drop_prob)\n",
    "\n",
    "    def forward(self, cw_idxs, qw_idxs):\n",
    "        c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n",
    "        q_mask = torch.zeros_like(qw_idxs) != qw_idxs\n",
    "        c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)\n",
    "\n",
    "        c_emb = self.emb(cw_idxs)         # (batch_size, c_len, hidden_size)\n",
    "        q_emb = self.emb(qw_idxs)         # (batch_size, q_len, hidden_size)\n",
    "\n",
    "        c_enc = self.enc(c_emb, c_len)    # (batch_size, c_len, 2 * hidden_size)\n",
    "        q_enc = self.enc(q_emb, q_len)    # (batch_size, q_len, 2 * hidden_size)\n",
    "\n",
    "        att = self.att(c_enc, q_enc,\n",
    "                       c_mask, q_mask)    # (batch_size, c_len, 8 * hidden_size)\n",
    "\n",
    "        mod = self.mod(att, c_len)        # (batch_size, c_len, 2 * hidden_size)\n",
    "\n",
    "        out = self.out(att, mod, c_mask)  # 2 tensors, each (batch_size, c_len)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU Option\n"
     ]
    }
   ],
   "source": [
    "model = BiDAF(word_vectors=word_vectors, hidden_size=100,\n",
    "             drop_prob=0.2)\n",
    "model - nn.DataParallel(model, gpu_ids) if len(gpu_ids) else print(\"no GPU Option\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiDAF(\n",
       "  (emb): Embedding(\n",
       "    (embed): Embedding(88746, 300)\n",
       "    (proj): Linear(in_features=300, out_features=100, bias=False)\n",
       "    (hwy): HighwayEncoder(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (1): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "      (gates): ModuleList(\n",
       "        (0): Linear(in_features=100, out_features=100, bias=True)\n",
       "        (1): Linear(in_features=100, out_features=100, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (enc): RNNEncoder(\n",
       "    (rnn): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (att): BiDAFAttention()\n",
       "  (mod): RNNEncoder(\n",
       "    (rnn): LSTM(800, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  )\n",
       "  (out): BiDAFOutput(\n",
       "    (att_linear_1): Linear(in_features=800, out_features=1, bias=True)\n",
       "    (mod_linear_1): Linear(in_features=200, out_features=1, bias=True)\n",
       "    (rnn): RNNEncoder(\n",
       "      (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "    (att_linear_2): Linear(in_features=800, out_features=1, bias=True)\n",
       "    (mod_linear_2): Linear(in_features=200, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "\n",
    "setting up the custom data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD(data.Dataset):\n",
    "    \"\"\"Stanford Question Answering Dataset (SQuAD).\n",
    "\n",
    "    Each item in the dataset is a tuple with the following entries (in order):\n",
    "        - context_idxs: Indices of the words in the context.\n",
    "            Shape (context_len,).\n",
    "        - context_char_idxs: Indices of the characters in the context.\n",
    "            Shape (context_len, max_word_len).\n",
    "        - question_idxs: Indices of the words in the question.\n",
    "            Shape (question_len,).\n",
    "        - question_char_idxs: Indices of the characters in the question.\n",
    "            Shape (question_len, max_word_len).\n",
    "        - y1: Index of word in the context where the answer begins.\n",
    "            -1 if no answer.\n",
    "        - y2: Index of word in the context where the answer ends.\n",
    "            -1 if no answer.\n",
    "        - id: ID of the example.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to .npz file containing pre-processed dataset.\n",
    "        use_v2 (bool): Whether to use SQuAD 2.0 questions. Otherwise only use SQuAD 1.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, use_v2=True):\n",
    "        super(SQuAD, self).__init__()\n",
    "\n",
    "        dataset = np.load(data_path)\n",
    "        self.context_idxs = torch.from_numpy(dataset['context_idxs']).long()\n",
    "        self.context_char_idxs = torch.from_numpy(dataset['context_char_idxs']).long()\n",
    "        self.question_idxs = torch.from_numpy(dataset['ques_idxs']).long()\n",
    "        self.question_char_idxs = torch.from_numpy(dataset['ques_char_idxs']).long()\n",
    "        self.y1s = torch.from_numpy(dataset['y1s']).long()\n",
    "        self.y2s = torch.from_numpy(dataset['y2s']).long()\n",
    "\n",
    "        if use_v2:\n",
    "            # SQuAD 2.0: Use index 0 for no-answer token (token 1 = OOV)\n",
    "            batch_size, c_len, w_len = self.context_char_idxs.size()\n",
    "            ones = torch.ones((batch_size, 1), dtype=torch.int64)\n",
    "            self.context_idxs = torch.cat((ones, self.context_idxs), dim=1)\n",
    "            self.question_idxs = torch.cat((ones, self.question_idxs), dim=1)\n",
    "\n",
    "            ones = torch.ones((batch_size, 1, w_len), dtype=torch.int64)\n",
    "            self.context_char_idxs = torch.cat((ones, self.context_char_idxs), dim=1)\n",
    "            self.question_char_idxs = torch.cat((ones, self.question_char_idxs), dim=1)\n",
    "\n",
    "            self.y1s += 1\n",
    "            self.y2s += 1\n",
    "\n",
    "        # SQuAD 1.1: Ignore no-answer examples\n",
    "        self.ids = torch.from_numpy(dataset['ids']).long()\n",
    "        self.valid_idxs = [idx for idx in range(len(self.ids))\n",
    "                           if use_v2 or self.y1s[idx].item() >= 0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.valid_idxs[idx]\n",
    "        example = (self.context_idxs[idx],\n",
    "                   self.context_char_idxs[idx],\n",
    "                   self.question_idxs[idx],\n",
    "                   self.question_char_idxs[idx],\n",
    "                   self.y1s[idx],\n",
    "                   self.y2s[idx],\n",
    "                   self.ids[idx])\n",
    "\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"Create batch tensors from a list of individual examples returned\n",
    "    by `SQuAD.__getitem__`. Merge examples of different length by padding\n",
    "    all examples to the maximum length in the batch.\n",
    "\n",
    "    Args:\n",
    "        examples (list): List of tuples of the form (context_idxs, context_char_idxs,\n",
    "        question_idxs, question_char_idxs, y1s, y2s, ids).\n",
    "\n",
    "    Returns:\n",
    "        examples (tuple): Tuple of tensors (context_idxs, context_char_idxs, question_idxs,\n",
    "        question_char_idxs, y1s, y2s, ids). All of shape (batch_size, ...), where\n",
    "        the remaining dimensions are the maximum length of examples in the input.\n",
    "\n",
    "    Adapted from:\n",
    "        https://github.com/yunjey/seq2seq-dataloader\n",
    "    \"\"\"\n",
    "    def merge_0d(scalars, dtype=torch.int64):\n",
    "        return torch.tensor(scalars, dtype=dtype)\n",
    "\n",
    "    def merge_1d(arrays, dtype=torch.int64, pad_value=0):\n",
    "        lengths = [(a != pad_value).sum() for a in arrays]\n",
    "        padded = torch.zeros(len(arrays), max(lengths), dtype=dtype)\n",
    "        for i, seq in enumerate(arrays):\n",
    "            end = lengths[i]\n",
    "            padded[i, :end] = seq[:end]\n",
    "        return padded\n",
    "\n",
    "    def merge_2d(matrices, dtype=torch.int64, pad_value=0):\n",
    "        heights = [(m.sum(1) != pad_value).sum() for m in matrices]\n",
    "        widths = [(m.sum(0) != pad_value).sum() for m in matrices]\n",
    "        padded = torch.zeros(len(matrices), max(heights), max(widths), dtype=dtype)\n",
    "        for i, seq in enumerate(matrices):\n",
    "            height, width = heights[i], widths[i]\n",
    "            padded[i, :height, :width] = seq[:height, :width]\n",
    "        return padded\n",
    "\n",
    "    # Group by tensor type\n",
    "    context_idxs, context_char_idxs, \\\n",
    "        question_idxs, question_char_idxs, \\\n",
    "        y1s, y2s, ids = zip(*examples)\n",
    "\n",
    "    # Merge into batch tensors\n",
    "    context_idxs = merge_1d(context_idxs)\n",
    "    context_char_idxs = merge_2d(context_char_idxs)\n",
    "    question_idxs = merge_1d(question_idxs)\n",
    "    question_char_idxs = merge_2d(question_char_idxs)\n",
    "    y1s = merge_0d(y1s)\n",
    "    y2s = merge_0d(y2s)\n",
    "    ids = merge_0d(ids)\n",
    "\n",
    "    return (context_idxs, context_char_idxs,\n",
    "            question_idxs, question_char_idxs,\n",
    "            y1s, y2s, ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SQuAD(\"./data_temp/train.npz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset,\n",
    "                               batch_size=64,\n",
    "                               shuffle=True,\n",
    "                               num_workers=4,\n",
    "                               collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = SQuAD(\"./data_temp/dev.npz\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = data.DataLoader(dev_dataset,\n",
    "                               batch_size=64,\n",
    "                               shuffle=False,\n",
    "                               num_workers=4,\n",
    "                               collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # explanation\n",
    "training! all building up to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "steps_till_eval_param = 50000\n",
    "steps_till_eval = steps_till_eval_param\n",
    "epoch = step // len(train_dataset)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointSaver:\n",
    "    \"\"\"Class to save and load model checkpoints.\n",
    "\n",
    "    Save the best checkpoints as measured by a metric value passed into the\n",
    "    `save` method. Overwrite checkpoints with better checkpoints once\n",
    "    `max_checkpoints` have been saved.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): Directory to save checkpoints.\n",
    "        max_checkpoints (int): Maximum number of checkpoints to keep before\n",
    "            overwriting old ones.\n",
    "        metric_name (str): Name of metric used to determine best model.\n",
    "        maximize_metric (bool): If true, best checkpoint is that which maximizes\n",
    "            the metric value passed in via `save`. Otherwise, best checkpoint\n",
    "            minimizes the metric.\n",
    "        log (logging.Logger): Optional logger for printing information.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_dir, max_checkpoints, metric_name,\n",
    "                 maximize_metric=False, log=None):\n",
    "        super(CheckpointSaver, self).__init__()\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        self.metric_name = metric_name\n",
    "        self.maximize_metric = maximize_metric\n",
    "        self.best_val = None\n",
    "        self.ckpt_paths = queue.PriorityQueue()\n",
    "        self.log = log\n",
    "        self._print('Saver will {}imize {}...'\n",
    "                    .format('max' if maximize_metric else 'min', metric_name))\n",
    "\n",
    "    def is_best(self, metric_val):\n",
    "        \"\"\"Check whether `metric_val` is the best seen so far.\n",
    "\n",
    "        Args:\n",
    "            metric_val (float): Metric value to compare to prior checkpoints.\n",
    "        \"\"\"\n",
    "        if metric_val is None:\n",
    "            # No metric reported\n",
    "            return False\n",
    "\n",
    "        if self.best_val is None:\n",
    "            # No checkpoint saved yet\n",
    "            return True\n",
    "\n",
    "        return ((self.maximize_metric and self.best_val < metric_val)\n",
    "                or (not self.maximize_metric and self.best_val > metric_val))\n",
    "\n",
    "    def _print(self, message):\n",
    "        \"\"\"Print a message if logging is enabled.\"\"\"\n",
    "        if self.log is not None:\n",
    "            self.log.info(message)\n",
    "    def save(self, step, model, metric_val, device):\n",
    "        \"\"\"Save model parameters to disk.\n",
    "\n",
    "        Args:\n",
    "            step (int): Total number of examples seen during training so far.\n",
    "            model (torch.nn.DataParallel): Model to save.\n",
    "            metric_val (float): Determines whether checkpoint is best so far.\n",
    "            device (torch.device): Device where model resides.\n",
    "        \"\"\"\n",
    "        ckpt_dict = {\n",
    "            'model_name': model.__class__.__name__,\n",
    "            'model_state': model.cpu().state_dict(),\n",
    "            'step': step\n",
    "        }\n",
    "        model.to(device)\n",
    "\n",
    "        checkpoint_path = os.path.join(self.save_dir,\n",
    "                                       'step_{}.pth.tar'.format(step))\n",
    "        torch.save(ckpt_dict, checkpoint_path)\n",
    "        self._print('Saved checkpoint: {}'.format(checkpoint_path))\n",
    "\n",
    "        if self.is_best(metric_val):\n",
    "            # Save the best model\n",
    "            self.best_val = metric_val\n",
    "            best_path = os.path.join(self.save_dir, 'best.pth.tar')\n",
    "            shutil.copy(checkpoint_path, best_path)\n",
    "            self._print('New best checkpoint at step {}...'.format(step))\n",
    "\n",
    "        # Add checkpoint path to priority queue (lowest priority removed first)\n",
    "        if self.maximize_metric:\n",
    "            priority_order = metric_val\n",
    "        else:\n",
    "            priority_order = -metric_val\n",
    "\n",
    "        self.ckpt_paths.put((priority_order, checkpoint_path))\n",
    "\n",
    "        # Remove a checkpoint if more than max_checkpoints have been saved\n",
    "        if self.ckpt_paths.qsize() > self.max_checkpoints:\n",
    "            _, worst_ckpt = self.ckpt_paths.get()\n",
    "            try:\n",
    "                os.remove(worst_ckpt)\n",
    "                self._print('Removed checkpoint: {}'.format(worst_ckpt))\n",
    "            except OSError:\n",
    "                # Avoid crashing if checkpoint has been removed or protected\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = CheckpointSaver(\"./save\",\n",
    "                             max_checkpoints=5,\n",
    "                             metric_name=\"F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, eval_file, max_len, use_squad_v2):\n",
    "    nll_meter = util.AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    pred_dict = {}\n",
    "    with open(eval_file, 'r') as fh:\n",
    "        gold_dict = json_load(fh)\n",
    "    with torch.no_grad(), \\\n",
    "            tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
    "        for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in data_loader:\n",
    "            # Setup for forward\n",
    "            cw_idxs = cw_idxs.to(device)\n",
    "            qw_idxs = qw_idxs.to(device)\n",
    "            batch_size = cw_idxs.size(0)\n",
    "\n",
    "            # Forward\n",
    "            log_p1, log_p2 = model(cw_idxs, qw_idxs)\n",
    "            y1, y2 = y1.to(device), y2.to(device)\n",
    "            loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n",
    "            nll_meter.update(loss.item(), batch_size)\n",
    "\n",
    "            # Get F1 and EM scores\n",
    "            p1, p2 = log_p1.exp(), log_p2.exp()\n",
    "            starts, ends = util.discretize(p1, p2, max_len, use_squad_v2)\n",
    "\n",
    "            # Log info\n",
    "            progress_bar.update(batch_size)\n",
    "            progress_bar.set_postfix(NLL=nll_meter.avg)\n",
    "\n",
    "            preds, _ = util.convert_tokens(gold_dict,\n",
    "                                           ids.tolist(),\n",
    "                                           starts.tolist(),\n",
    "                                           ends.tolist(),\n",
    "                                           use_squad_v2)\n",
    "            pred_dict.update(preds)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    results = util.eval_dicts(gold_dict, pred_dict, use_squad_v2)\n",
    "    results_list = [('NLL', nll_meter.avg),\n",
    "                    ('F1', results['F1']),\n",
    "                    ('EM', results['EM'])]\n",
    "    if use_squad_v2:\n",
    "        results_list.append(('AvNA', results['AvNA']))\n",
    "    results = OrderedDict(results_list)\n",
    "\n",
    "    return results, pred_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
