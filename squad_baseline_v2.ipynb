{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "  - Install any required packages (use the `set_up.sh` script)\n",
    "  - Set up all of the imports\n",
    "  - set up the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import urllib3\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import ujson\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 3716 # setting the random seed for consistent runs\n",
    "\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed_all(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "This section is to import and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data = False\n",
    "# if this flag is set to true, then the data will be downloaded\n",
    "# otherwise this step will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download(url=\"https://google.com/\", file_name=\"default.json\", file_path=\"/tmp/\"):\n",
    "    with http.request('GET', url, preload_content=False) as r, open((file_path + file_name), 'wb') as out_file:       \n",
    "            shutil.copyfileobj(r, out_file)\n",
    "    return (\"the\", file_name, \"was downloaded to\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_DATA_PATH = os.getcwd() + \"/data_v2/\" \n",
    "\n",
    "TRAIN_DATA_PATH = GEN_DATA_PATH + \"train/\" \n",
    "DEV_DATA_PATH = GEN_DATA_PATH + \"dev/\"\n",
    "\n",
    "\n",
    "if import_data:\n",
    "    \n",
    "    print(\"LOG: data import started\")\n",
    "\n",
    "    SQUAD_DATA_TRAINING = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "    SQUAD_DATA_DEV = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "    \n",
    "    http = urllib3.PoolManager()\n",
    "    \n",
    "    print(\"LOG: starting train data download process\")\n",
    "    data_download(url=SQUAD_DATA_TRAINING, file_name=\"train_raw_data.json\", file_path=TRAIN_DATA_PATH)\n",
    "    print(\"LOG: starting dev data download process\")\n",
    "    data_download(url=SQUAD_DATA_DEV, file_name=\"dev_raw_data.json\", file_path=DEV_DATA_PATH)\n",
    "    \n",
    "    print(\"LOG: finished data importing\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data downloaded, we need to do prepare it for training. Luckily, we don't need too much for this datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_json('data_v2/dev/dev_raw_data.json')\n",
    "train = pd.read_json('data_v2/train/train_raw_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dev raw shape:\", dev.shape)\n",
    "print(\"train raw shape:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we only care about the data itself at this point \n",
    "# since we know it is version 2\n",
    "\n",
    "train = train['data']\n",
    "dev = dev['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "answers_text = []\n",
    "answers_starti = []\n",
    "impossibility = []\n",
    "topics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(qac, context, title):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset):\n",
    "    for topic in dataset:\n",
    "        print(topic['title'])\n",
    "        for paragraph in topic['paragraphs']:\n",
    "            for qac in paragraph['qas']:\n",
    "                \n",
    "                topics.append(topic['title'])\n",
    "                questions.append(qac['question'])\n",
    "                if qac['is_impossible'] == \"False\":\n",
    "                    answers_text.append(qac['answers'][0]['text']) \n",
    "                    answers_starti.append(qac['answers'][0]['answer_start'])\n",
    "                    impossibility.append(False)\n",
    "\n",
    "                else:\n",
    "                    answers_text.append(\"\")\n",
    "                    answers_starti.append(\"\")\n",
    "                    impossibility.append(True)\n",
    "                \n",
    "                contexts.append(paragraph['context'])\n",
    "                \n",
    "    assembled_df = pd.DataFrame({\"context\":contexts, \n",
    "                                 \"question\": questions, \n",
    "                                 \"answer_start\": answers_starti, \n",
    "                                 \"answers_text\": answers_text,\n",
    "                                 \"impossibility\": impossibility,\n",
    "                                 \"topic\": topics})\n",
    "    \n",
    "    return assembled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembled_df = process_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assembled_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# - clean the data via the cleaner function\n",
    "# - define the custom pytorch data structure\n",
    "# - add word/vector embeddings\n",
    "# - implement a basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Create the different models that will be used as a part of the baseline training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section will contain all code used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
