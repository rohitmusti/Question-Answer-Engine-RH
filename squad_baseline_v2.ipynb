{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "  - Install any required packages (use the `set_up.sh` script)\n",
    "  - Set up all of the imports\n",
    "  - set up the random seed\n",
    "  \n",
    "##### DISCLAIMER: This heavily borrows from [src](https://github.com/HKUST-KnowComp/R-Net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import urllib3\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import ujson\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 3716 # setting the random seed for consistent runs\n",
    "\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed_all(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "This section is to import and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this flag is set to true, then the data will be downloaded\n",
    "# otherwise this step will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f5ee76fc009a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtoolkit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/active/stanford_test/toolkit.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m def squad_data_import(data_path=\"./data\", train_link=\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\",\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import toolkit as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        config.train_file = \"./data/train_raw_data.json\"\n",
    "        config.dev_file = \"./data/dev_raw_data.json\"\n",
    "        config.glove_word_file = \"./data/glove.840B.300d.txt\"\n",
    "        config.glove_char_file = \"./data/glove.840B.300d-char.txt\"\n",
    "        config.glove_char_size = 94\n",
    "        config.glove_word_size = int(2.2e6)\n",
    "        config.glove_dim = 300\n",
    "        config.char_dim = 8\n",
    "        config.train_record_file = \"data/train.tfrecords\"\n",
    "        config.dev_record_file = \"data/dev.tfrecords\"\n",
    "        config.word_emb_file = \"data/word_embeddings.embd\"\n",
    "        config.char_emb_file = \"data/char_embeddings.embd\"\n",
    "        config.dev_meta = \"data/dev_meta.meta\"\n",
    "        config.para_limit = 400\n",
    "        config.ques_limit = 50\n",
    "        config.char_limit = 16\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tk.prepro(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO: \n",
    " - Pray that this works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Create the different models that will be used as a part of the baseline training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Assortment of layers for use in models.py.\n",
    "Author:\n",
    "    Chris Chute (chute@stanford.edu)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# from util import masked_softmax\n",
    "\n",
    "def masked_softmax(logits, mask, dim=-1, log_softmax=False):\n",
    "    \"\"\"Take the softmax of `logits` over given dimension, and set\n",
    "    entries to 0 wherever `mask` is 0.\n",
    "    Args:\n",
    "        logits (torch.Tensor): Inputs to the softmax function.\n",
    "        mask (torch.Tensor): Same shape as `logits`, with 0 indicating\n",
    "            positions that should be assigned 0 probability in the output.\n",
    "        dim (int): Dimension over which to take softmax.\n",
    "        log_softmax (bool): Take log-softmax rather than regular softmax.\n",
    "            E.g., some PyTorch functions such as `F.nll_loss` expect log-softmax.\n",
    "    Returns:\n",
    "        probs (torch.Tensor): Result of taking masked softmax over the logits.\n",
    "    \"\"\"\n",
    "    mask = mask.type(torch.float32)\n",
    "    masked_logits = mask * logits + (1 - mask) * -1e30\n",
    "    softmax_fn = F.log_softmax if log_softmax else F.softmax\n",
    "    probs = softmax_fn(masked_logits, dim)\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    \"\"\"Embedding layer used by BiDAF, without the character-level component.\n",
    "    Word-level embeddings are further refined using a 2-layer Highway Encoder\n",
    "    (see `HighwayEncoder` class for details).\n",
    "    Args:\n",
    "        word_vectors (torch.Tensor): Pre-trained word vectors.\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "        drop_prob (float): Probability of zero-ing out activations\n",
    "    \"\"\"\n",
    "    def __init__(self, word_vectors, hidden_size, drop_prob):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.embed = nn.Embedding.from_pretrained(word_vectors)\n",
    "        self.proj = nn.Linear(word_vectors.size(1), hidden_size, bias=False)\n",
    "        self.hwy = HighwayEncoder(2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embed(x)   # (batch_size, seq_len, embed_size)\n",
    "        emb = F.dropout(emb, self.drop_prob, self.training)\n",
    "        emb = self.proj(emb)  # (batch_size, seq_len, hidden_size)\n",
    "        emb = self.hwy(emb)   # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return emb\n",
    "\n",
    "\n",
    "class HighwayEncoder(nn.Module):\n",
    "    \"\"\"Encode an input sequence using a highway network.\n",
    "    Based on the paper:\n",
    "    \"Highway Networks\"\n",
    "    by Rupesh Kumar Srivastava, Klaus Greff, JÃ¼rgen Schmidhuber\n",
    "    (https://arxiv.org/abs/1505.00387).\n",
    "    Args:\n",
    "        num_layers (int): Number of layers in the highway encoder.\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, hidden_size):\n",
    "        super(HighwayEncoder, self).__init__()\n",
    "        self.transforms = nn.ModuleList([nn.Linear(hidden_size, hidden_size)\n",
    "                                         for _ in range(num_layers)])\n",
    "        self.gates = nn.ModuleList([nn.Linear(hidden_size, hidden_size)\n",
    "                                    for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for gate, transform in zip(self.gates, self.transforms):\n",
    "            # Shapes of g, t, and x are all (batch_size, seq_len, hidden_size)\n",
    "            g = torch.sigmoid(gate(x))\n",
    "            t = F.relu(transform(x))\n",
    "            x = g * t + (1 - g) * x\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNNEncoder(nn.Module):\n",
    "    \"\"\"General-purpose layer for encoding a sequence using a bidirectional RNN.\n",
    "    Encoded output is the RNN's hidden state at each position, which\n",
    "    has shape `(batch_size, seq_len, hidden_size * 2)`.\n",
    "    Args:\n",
    "        input_size (int): Size of a single timestep in the input.\n",
    "        hidden_size (int): Size of the RNN hidden state.\n",
    "        num_layers (int): Number of layers of RNN cells to use.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers,\n",
    "                 drop_prob=0.):\n",
    "        super(RNNEncoder, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True,\n",
    "                           dropout=drop_prob if num_layers > 1 else 0.)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Save original padded length for use by pad_packed_sequence\n",
    "        orig_len = x.size(1)\n",
    "\n",
    "        # Sort by length and pack sequence for RNN\n",
    "        lengths, sort_idx = lengths.sort(0, descending=True)\n",
    "        x = x[sort_idx]     # (batch_size, seq_len, input_size)\n",
    "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "\n",
    "        # Apply RNN\n",
    "        x, _ = self.rnn(x)  # (batch_size, seq_len, 2 * hidden_size)\n",
    "\n",
    "        # Unpack and reverse sort\n",
    "        x, _ = pad_packed_sequence(x, batch_first=True, total_length=orig_len)\n",
    "        _, unsort_idx = sort_idx.sort(0)\n",
    "        x = x[unsort_idx]   # (batch_size, seq_len, 2 * hidden_size)\n",
    "\n",
    "        # Apply dropout (RNN applies dropout after all but the last layer)\n",
    "        x = F.dropout(x, self.drop_prob, self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BiDAFAttention(nn.Module):\n",
    "    \"\"\"Bidirectional attention originally used by BiDAF.\n",
    "    Bidirectional attention computes attention in two directions:\n",
    "    The context attends to the query and the query attends to the context.\n",
    "    The output of this layer is the concatenation of [context, c2q_attention,\n",
    "    context * c2q_attention, context * q2c_attention]. This concatenation allows\n",
    "    the attention vector at each timestep, along with the embeddings from\n",
    "    previous layers, to flow through the attention layer to the modeling layer.\n",
    "    The output has shape (batch_size, context_len, 8 * hidden_size).\n",
    "    Args:\n",
    "        hidden_size (int): Size of hidden activations.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, drop_prob=0.1):\n",
    "        super(BiDAFAttention, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.c_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.q_weight = nn.Parameter(torch.zeros(hidden_size, 1))\n",
    "        self.cq_weight = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        for weight in (self.c_weight, self.q_weight, self.cq_weight):\n",
    "            nn.init.xavier_uniform_(weight)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, c, q, c_mask, q_mask):\n",
    "        batch_size, c_len, _ = c.size()\n",
    "        q_len = q.size(1)\n",
    "        s = self.get_similarity_matrix(c, q)        # (batch_size, c_len, q_len)\n",
    "        c_mask = c_mask.view(batch_size, c_len, 1)  # (batch_size, c_len, 1)\n",
    "        q_mask = q_mask.view(batch_size, 1, q_len)  # (batch_size, 1, q_len)\n",
    "        s1 = masked_softmax(s, q_mask, dim=2)       # (batch_size, c_len, q_len)\n",
    "        s2 = masked_softmax(s, c_mask, dim=1)       # (batch_size, c_len, q_len)\n",
    "\n",
    "        # (bs, c_len, q_len) x (bs, q_len, hid_size) => (bs, c_len, hid_size)\n",
    "        a = torch.bmm(s1, q)\n",
    "        # (bs, c_len, c_len) x (bs, c_len, hid_size) => (bs, c_len, hid_size)\n",
    "        b = torch.bmm(torch.bmm(s1, s2.transpose(1, 2)), c)\n",
    "\n",
    "        x = torch.cat([c, a, c * a, c * b], dim=2)  # (bs, c_len, 4 * hid_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_similarity_matrix(self, c, q):\n",
    "        \"\"\"Get the \"similarity matrix\" between context and query (using the\n",
    "        terminology of the BiDAF paper).\n",
    "        A naive implementation as described in BiDAF would concatenate the\n",
    "        three vectors then project the result with a single weight matrix. This\n",
    "        method is a more memory-efficient implementation of the same operation.\n",
    "        See Also:\n",
    "            Equation 1 in https://arxiv.org/abs/1611.01603\n",
    "        \"\"\"\n",
    "        c_len, q_len = c.size(1), q.size(1)\n",
    "        c = F.dropout(c, self.drop_prob, self.training)  # (bs, c_len, hid_size)\n",
    "        q = F.dropout(q, self.drop_prob, self.training)  # (bs, q_len, hid_size)\n",
    "\n",
    "        # Shapes: (batch_size, c_len, q_len)\n",
    "        s0 = torch.matmul(c, self.c_weight).expand([-1, -1, q_len])\n",
    "        s1 = torch.matmul(q, self.q_weight).transpose(1, 2)\\\n",
    "                                           .expand([-1, c_len, -1])\n",
    "        s2 = torch.matmul(c * self.cq_weight, q.transpose(1, 2))\n",
    "        s = s0 + s1 + s2 + self.bias\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "class BiDAFOutput(nn.Module):\n",
    "    \"\"\"Output layer used by BiDAF for question answering.\n",
    "    Computes a linear transformation of the attention and modeling\n",
    "    outputs, then takes the softmax of the result to get the start pointer.\n",
    "    A bidirectional LSTM is then applied the modeling output to produce `mod_2`.\n",
    "    A second linear+softmax of the attention output and `mod_2` is used\n",
    "    to get the end pointer.\n",
    "    Args:\n",
    "        hidden_size (int): Hidden size used in the BiDAF model.\n",
    "        drop_prob (float): Probability of zero-ing out activations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, drop_prob):\n",
    "        super(BiDAFOutput, self).__init__()\n",
    "        self.att_linear_1 = nn.Linear(8 * hidden_size, 1)\n",
    "        self.mod_linear_1 = nn.Linear(2 * hidden_size, 1)\n",
    "\n",
    "        self.rnn = RNNEncoder(input_size=2 * hidden_size,\n",
    "                              hidden_size=hidden_size,\n",
    "                              num_layers=1,\n",
    "                              drop_prob=drop_prob)\n",
    "\n",
    "        self.att_linear_2 = nn.Linear(8 * hidden_size, 1)\n",
    "        self.mod_linear_2 = nn.Linear(2 * hidden_size, 1)\n",
    "\n",
    "    def forward(self, att, mod, mask):\n",
    "        # Shapes: (batch_size, seq_len, 1)\n",
    "        logits_1 = self.att_linear_1(att) + self.mod_linear_1(mod)\n",
    "        mod_2 = self.rnn(mod, mask.sum(-1))\n",
    "        logits_2 = self.att_linear_2(att) + self.mod_linear_2(mod_2)\n",
    "\n",
    "        # Shapes: (batch_size, seq_len)\n",
    "        log_p1 = masked_softmax(logits_1.squeeze(), mask, log_softmax=True)\n",
    "        log_p2 = masked_softmax(logits_2.squeeze(), mask, log_softmax=True)\n",
    "\n",
    "        return log_p1, log_p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Top-level model classes.\n",
    "Author:\n",
    "    Chris Chute (chute@stanford.edu)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BiDAF(nn.Module):\n",
    "    \"\"\"Baseline BiDAF model for SQuAD.\n",
    "    Based on the paper:\n",
    "    \"Bidirectional Attention Flow for Machine Comprehension\"\n",
    "    by Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, Hannaneh Hajishirzi\n",
    "    (https://arxiv.org/abs/1611.01603).\n",
    "    Follows a high-level structure commonly found in SQuAD models:\n",
    "        - Embedding layer: Embed word indices to get word vectors.\n",
    "        - Encoder layer: Encode the embedded sequence.\n",
    "        - Attention layer: Apply an attention mechanism to the encoded sequence.\n",
    "        - Model encoder layer: Encode the sequence again.\n",
    "        - Output layer: Simple layer (e.g., fc + softmax) to get final outputs.\n",
    "    Args:\n",
    "        word_vectors (torch.Tensor): Pre-trained word vectors.\n",
    "        hidden_size (int): Number of features in the hidden state at each layer.\n",
    "        drop_prob (float): Dropout probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_vectors, hidden_size, drop_prob=0.):\n",
    "        super(BiDAF, self).__init__()\n",
    "        self.emb = Embedding(word_vectors=word_vectors,\n",
    "                                    hidden_size=hidden_size,\n",
    "                                    drop_prob=drop_prob)\n",
    "\n",
    "        self.enc = RNNEncoder(input_size=hidden_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     num_layers=1,\n",
    "                                     drop_prob=drop_prob)\n",
    "\n",
    "        self.att = BiDAFAttention(hidden_size=2 * hidden_size,\n",
    "                                         drop_prob=drop_prob)\n",
    "\n",
    "        self.mod = RNNEncoder(input_size=8 * hidden_size,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     num_layers=2,\n",
    "                                     drop_prob=drop_prob)\n",
    "\n",
    "        self.out = BiDAFOutput(hidden_size=hidden_size,\n",
    "                                      drop_prob=drop_prob)\n",
    "\n",
    "    def forward(self, cw_idxs, qw_idxs):\n",
    "        c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n",
    "        q_mask = torch.zeros_like(qw_idxs) != qw_idxs\n",
    "        c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)\n",
    "\n",
    "        c_emb = self.emb(cw_idxs)         # (batch_size, c_len, hidden_size)\n",
    "        q_emb = self.emb(qw_idxs)         # (batch_size, q_len, hidden_size)\n",
    "\n",
    "        c_enc = self.enc(c_emb, c_len)    # (batch_size, c_len, 2 * hidden_size)\n",
    "        q_enc = self.enc(q_emb, q_len)    # (batch_size, q_len, 2 * hidden_size)\n",
    "\n",
    "        att = self.att(c_enc, q_enc,\n",
    "                       c_mask, q_mask)    # (batch_size, c_len, 8 * hidden_size)\n",
    "\n",
    "        mod = self.mod(att, c_len)        # (batch_size, c_len, 2 * hidden_size)\n",
    "\n",
    "        out = self.out(att, mod, c_mask)  # 2 tensors, each (batch_size, c_len)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD(data.Dataset):\n",
    "    \"\"\"Stanford Question Answering Dataset (SQuAD).\n",
    "    Each item in the dataset is a tuple with the following entries (in order):\n",
    "        - context_idxs: Indices of the words in the context.\n",
    "            Shape (context_len,).\n",
    "        - context_char_idxs: Indices of the characters in the context.\n",
    "            Shape (context_len, max_word_len).\n",
    "        - question_idxs: Indices of the words in the question.\n",
    "            Shape (question_len,).\n",
    "        - question_char_idxs: Indices of the characters in the question.\n",
    "            Shape (question_len, max_word_len).\n",
    "        - y1: Index of word in the context where the answer begins.\n",
    "            -1 if no answer.\n",
    "        - y2: Index of word in the context where the answer ends.\n",
    "            -1 if no answer.\n",
    "        - id: ID of the example.\n",
    "    Args:\n",
    "        data_path (str): Path to .npz file containing pre-processed dataset.\n",
    "        use_v2 (bool): Whether to use SQuAD 2.0 questions. Otherwise only use SQuAD 1.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, use_v2=True):\n",
    "        super(SQuAD, self).__init__()\n",
    "        \n",
    "#        with open(data_path) as file_stream:\n",
    "#            ujson.load(file_stream)\n",
    "\n",
    "        dataset = np.load(data_path)\n",
    "        self.context_idxs = torch.from_numpy(dataset['context_idxs']).long()\n",
    "        self.context_char_idxs = torch.from_numpy(dataset['context_char_idxs']).long()\n",
    "        self.question_idxs = torch.from_numpy(dataset['ques_idxs']).long()\n",
    "        self.question_char_idxs = torch.from_numpy(dataset['ques_char_idxs']).long()\n",
    "        self.y1s = torch.from_numpy(dataset['y1s']).long()\n",
    "        self.y2s = torch.from_numpy(dataset['y2s']).long()\n",
    "\n",
    "        if use_v2:\n",
    "            # SQuAD 2.0: Use index 0 for no-answer token (token 1 = OOV)\n",
    "            batch_size, c_len, w_len = self.context_char_idxs.size()\n",
    "            ones = torch.ones((batch_size, 1), dtype=torch.int64)\n",
    "            self.context_idxs = torch.cat((ones, self.context_idxs), dim=1)\n",
    "            self.question_idxs = torch.cat((ones, self.question_idxs), dim=1)\n",
    "\n",
    "            ones = torch.ones((batch_size, 1, w_len), dtype=torch.int64)\n",
    "            self.context_char_idxs = torch.cat((ones, self.context_char_idxs), dim=1)\n",
    "            self.question_char_idxs = torch.cat((ones, self.question_char_idxs), dim=1)\n",
    "\n",
    "            self.y1s += 1\n",
    "            self.y2s += 1\n",
    "\n",
    "        # SQuAD 1.1: Ignore no-answer examples\n",
    "        self.ids = torch.from_numpy(dataset['ids']).long()\n",
    "        self.valid_idxs = [idx for idx in range(len(self.ids))\n",
    "                           if use_v2 or self.y1s[idx].item() >= 0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.valid_idxs[idx]\n",
    "        example = (self.context_idxs[idx],\n",
    "                   self.context_char_idxs[idx],\n",
    "                   self.question_idxs[idx],\n",
    "                   self.question_char_idxs[idx],\n",
    "                   self.y1s[idx],\n",
    "                   self.y2s[idx],\n",
    "                   self.ids[idx])\n",
    "\n",
    "        return example\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_idxs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section will contain all code used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_from_json(path, dtype=torch.float32):\n",
    "    \"\"\"Load a PyTorch Tensor from a JSON file.\n",
    "    Args:\n",
    "        path (str): Path to the JSON file to load.\n",
    "        dtype (torch.dtype): Data type of loaded array.\n",
    "    Returns:\n",
    "        tensor (torch.Tensor): Tensor loaded from JSON file.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as fh:\n",
    "        array = np.array(json.load(fh))\n",
    "\n",
    "    tensor = torch.from_numpy(array).type(dtype)\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = torch_from_json(\"data_v2/embeddings/word_emb.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiDAF(word_vectors=word_vectors,\n",
    "                  hidden_size=100,\n",
    "                  drop_prob=0.2)\n",
    "\n",
    "# model = nn.DataParallel(model, device[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(model.parameters(), 0.5,\n",
    "                               weight_decay=0)\n",
    "scheduler = sched.LambdaLR(optimizer, lambda s: 1.)  # Constant LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loader\n",
    "train_dataset = SQuAD(\"./data_v2/train/train.npz\", True)\n",
    "train_loader = data.DataLoader(train_dataset,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=True,\n",
    "                               num_workers=args.num_workers,\n",
    "                               collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
