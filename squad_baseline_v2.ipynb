{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "  - Install any required packages (use the `set_up.sh` script)\n",
    "  - Set up all of the imports\n",
    "  - set up the random seed\n",
    "  \n",
    "##### DISCLAIMER: This heavily borrows from [here](https://github.com/chrischute/squad/blob/master/setup.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import urllib3\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import ujson\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 3716 # setting the random seed for consistent runs\n",
    "\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed_all(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "This section is to import and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data = False\n",
    "# if this flag is set to true, then the data will be downloaded\n",
    "# otherwise this step will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download(url=\"https://google.com/\", file_name=\"default.json\", file_path=\"/tmp/\"):\n",
    "    with http.request('GET', url, preload_content=False) as r, open((file_path + file_name), 'wb') as out_file:       \n",
    "            shutil.copyfileobj(r, out_file)\n",
    "    return (\"the\", file_name, \"was downloaded to\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_DATA_PATH = os.getcwd() + \"/data_v2/\" \n",
    "\n",
    "TRAIN_DATA_PATH = GEN_DATA_PATH + \"train/\" \n",
    "DEV_DATA_PATH = GEN_DATA_PATH + \"dev/\"\n",
    "\n",
    "\n",
    "if import_data:\n",
    "    \n",
    "    print(\"LOG: data import started\")\n",
    "\n",
    "    SQUAD_DATA_TRAINING = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "    SQUAD_DATA_DEV = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "    \n",
    "    http = urllib3.PoolManager()\n",
    "    \n",
    "    print(\"LOG: starting train data download process\")\n",
    "    data_download(url=SQUAD_DATA_TRAINING, file_name=\"train_raw_data.json\", file_path=TRAIN_DATA_PATH)\n",
    "    print(\"LOG: starting dev data download process\")\n",
    "    data_download(url=SQUAD_DATA_DEV, file_name=\"dev_raw_data.json\", file_path=DEV_DATA_PATH)\n",
    "    \n",
    "    print(\"LOG: finished data importing\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data downloaded, we need to do prepare it for training. Luckily, we don't need too much for this datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_json('data_v2/dev/dev_raw_data.json')\n",
    "train = pd.read_json('data_v2/train/train_raw_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dev raw shape:\", dev.shape)\n",
    "print(\"train raw shape:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we only care about the data itself at this point \n",
    "# since we know it is version 2\n",
    "\n",
    "train = train['data']\n",
    "dev = dev['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "answers_text = []\n",
    "answers_starti = []\n",
    "impossibility = []\n",
    "topics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(item):\n",
    "    return item.replace(\"''\", '\" ').replace(\"``\", '\" ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_cleaner(context):\n",
    "    context_ret = standard(context)\n",
    "    context_tokens = word_tokenize(context_ret)\n",
    "    spans = convert_idx(context_ret,context_tokens)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "    \n",
    "    return context_ret, context_tokens, spans, context_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(context_tokens, word_counts, char_counts):\n",
    "    \n",
    "    \n",
    "            # note: \n",
    "            # including unique char + word counting here will\n",
    "            # help assess the problem space and inform model design choices\n",
    "            \n",
    "            # originally was += len(paragraph[\"qas\"]) for word + char\n",
    "            # why???\n",
    "            \n",
    "    for token in context_tokens:\n",
    "                    word_counts[token] += 1\n",
    "                    for char in token:\n",
    "                        char_counts[char] += 1    \n",
    "    return word_counts, char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(qac, context):\n",
    "    question_ret = qac['question']\n",
    "    answer_text_ret = qac['answers'][0]['text']\n",
    "    answer_starti_ret= qac['answers'][0]['answer_start']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset, word_counts, char_counts):\n",
    "    for topic in dataset:\n",
    "        topic_add = topic['title']\n",
    "        print(topic_add)\n",
    "        for paragraph in topic['paragraphs']:            \n",
    "            context_cl, context_tokens, spans, context_chars = context_cleaner(paragraph[\"context\"])\n",
    "            \n",
    "            \n",
    "            word_counts, char_counts = get_counts(context_tokens, word_counts, char_counts)\n",
    "            \n",
    "                   \n",
    "            \n",
    "            \n",
    "            for qac in paragraph['qas']:\n",
    "                topics.append(topic_add)\n",
    "                questions.append(qac['question'])\n",
    "                if qac['is_impossible'] == \"False\":\n",
    "                    answers_text.append() \n",
    "                    answers_starti.append()\n",
    "                    impossibility.append(False)\n",
    "\n",
    "                else:\n",
    "                    answers_text.append(\"\")\n",
    "                    answers_starti.append(\"\")\n",
    "                    impossibility.append(True)\n",
    "                \n",
    "                contexts.append()\n",
    "                \n",
    "    assembled_df = pd.DataFrame({\"context\":contexts, \n",
    "                                 \"question\": questions, \n",
    "                                 \"answer_start\": answers_starti, \n",
    "                                 \"answers_text\": answers_text,\n",
    "                                 \"impossibility\": impossibility,\n",
    "                                 \"topic\": topics})\n",
    "    \n",
    "    return assembled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "char_counts = {}\n",
    "assembled_df = process_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assembled_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# - clean the data via the cleaner function\n",
    "# - define the custom pytorch data structure\n",
    "# - add word/vector embeddings\n",
    "# - implement a basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Create the different models that will be used as a part of the baseline training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section will contain all code used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
