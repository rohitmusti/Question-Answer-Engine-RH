{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "  - Install any required packages (use the `set_up.sh` script)\n",
    "  - Set up all of the imports\n",
    "  - set up the random seed\n",
    "  \n",
    "##### DISCLAIMER: This heavily borrows from [src](https://github.com/chrischute/squad/blob/master/setup.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import urllib3\n",
    "import spacy\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import ujson\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 3716 # setting the random seed for consistent runs\n",
    "\n",
    "random.seed(rand_seed)\n",
    "np.random.seed(rand_seed)\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed_all(rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "This section is to import and pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data = False\n",
    "# if this flag is set to true, then the data will be downloaded\n",
    "# otherwise this step will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download(url=\"https://google.com/\", file_name=\"default.json\", file_path=\"/tmp/\"):\n",
    "    with http.request('GET', url, preload_content=False) as r, open((file_path + file_name), 'wb') as out_file:       \n",
    "            shutil.copyfileobj(r, out_file)\n",
    "    return (\"the\", file_name, \"was downloaded to\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_DATA_PATH = os.getcwd() + \"/data_v2/\" \n",
    "\n",
    "TRAIN_DATA_PATH = GEN_DATA_PATH + \"train/\" \n",
    "DEV_DATA_PATH = GEN_DATA_PATH + \"dev/\"\n",
    "\n",
    "\n",
    "if import_data:\n",
    "    \n",
    "    print(\"LOG: data import started\")\n",
    "\n",
    "    SQUAD_DATA_TRAINING = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "    SQUAD_DATA_DEV = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
    "    \n",
    "    http = urllib3.PoolManager()\n",
    "    \n",
    "    print(\"LOG: starting train data download process\")\n",
    "    data_download(url=SQUAD_DATA_TRAINING, file_name=\"train_raw_data.json\", file_path=TRAIN_DATA_PATH)\n",
    "    print(\"LOG: starting dev data download process\")\n",
    "    data_download(url=SQUAD_DATA_DEV, file_name=\"dev_raw_data.json\", file_path=DEV_DATA_PATH)\n",
    "    \n",
    "    print(\"LOG: finished data importing\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data downloaded, we need to do prepare it for training. Luckily, we don't need too much for this datatset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_json('data_v2/dev/dev_raw_data.json')\n",
    "train = pd.read_json('data_v2/train/train_raw_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev raw shape: (35, 2)\n",
      "train raw shape: (442, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dev raw shape:\", dev.shape)\n",
    "print(\"train raw shape:\", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we only care about the data itself at this point \n",
    "# since we know it is version 2\n",
    "\n",
    "train = train['data']\n",
    "dev = dev['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(item):\n",
    "    return item.replace(\"''\", '\" ').replace(\"``\", '\" ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    doc = nlp(sent)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_idx(text, tokens):\n",
    "    current = 0\n",
    "    spans = []\n",
    "    for token in tokens:\n",
    "        current = text.find(token, current)\n",
    "        if current < 0:\n",
    "            print(\"Token {} cannot be found\".format(token))\n",
    "            raise Exception()\n",
    "        spans.append((current, current + len(token)))\n",
    "        current += len(token)\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_cleaner(context):\n",
    "    context_ret = standardize(context)\n",
    "    context_tokens = word_tokenize(context_ret)\n",
    "    spans = convert_idx(context_ret,context_tokens)\n",
    "    context_chars = [list(token) for token in context_tokens]\n",
    "    \n",
    "    return context_ret, context_tokens, spans, context_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(tokens, word_counts, char_counts):\n",
    "    \n",
    "    \n",
    "    # note: \n",
    "    # including unique char + word counting here will\n",
    "    # help assess the problem space and inform model design choices\n",
    "    \n",
    "    # originally was += len(paragraph[\"qas\"]) for word & char\n",
    "    # why???\n",
    "    \n",
    "    for token in tokens:\n",
    "        word_counts[token] += 1 \n",
    "        for char in token:\n",
    "            char_counts[token] += 1\n",
    "\n",
    "    return word_counts, char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_cleaner(qac):\n",
    "    question = standardize(qac['question'])\n",
    "    question_tokens = word_tokenize(question)\n",
    "    question_chars = [list(token) for token in question_tokens]\n",
    "    answer_text = \"\"\n",
    "    answer_starti = -1\n",
    "    if qac['is_impossible'] == \"False\":\n",
    "        answer_text = standardize(qac['answers'][0]['text'])\n",
    "        answer_starti = qac['answers'][0]['answer_start']\n",
    "    \n",
    "    return question, answer_text, answer_starti, question_tokens, question_chars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset, word_counts, char_counts):\n",
    "    total = 0\n",
    "    examples = []\n",
    "    eval_examples = {}\n",
    "    for topic in dataset:\n",
    "        topic_add = topic['title']\n",
    "        for paragraph in topic['paragraphs']:            \n",
    "            \n",
    "            context, context_tokens, spans, context_chars = context_cleaner(paragraph[\"context\"])\n",
    "            word_counts, char_counts = get_counts(context_tokens, word_counts, char_counts)\n",
    "            \n",
    "            for qac in paragraph['qas']:\n",
    "                total += 1\n",
    "                question, answer_text, answer_starti, question_tokens, question_chars = question_cleaner(qac)\n",
    "                word_counts, char_counts = get_counts(question_tokens, word_counts, char_counts)\n",
    "                \n",
    "                y1s, y2s = [], []\n",
    "                answer_span = []\n",
    "                for idx, span, in enumerate(spans):\n",
    "                    if not ((answer_starti + len(answer_text) <= span[1]) or answer_starti >= span[1]):\n",
    "                        answer_span.append(idx)\n",
    "                \n",
    "                if (len(answer_span) > 0):\n",
    "                    y1, y2 = answer_span[0], answer_span[-1]\n",
    "                    y1s.append(y1)\n",
    "                    y2s.append(y2)\n",
    "                \n",
    "                example = {\"context_tokens\": context_tokens,\n",
    "                               \"context_chars\": context_chars,\n",
    "                               \"ques_tokens\": question_tokens,\n",
    "                               \"ques_chars\": question_chars,\n",
    "                               \"y1s\": y1s,\n",
    "                               \"y2s\": y2s,\n",
    "                               \"id\": total}\n",
    "                \n",
    "                examples.append(example)\n",
    "                eval_examples[str(total)] = {\"topic\": topic_add,\n",
    "                                             \"context\": context,\n",
    "                                             \"question\": question,\n",
    "                                             \"spans\": spans,\n",
    "                                             \"answers\": answer_text,\n",
    "                                             \"uuid\": qac[\"id\"]} \n",
    "                sys.stdout.write(\"Processed: %d examples\\r\" % (total))\n",
    "                sys.stdout.flush()\n",
    "                 \n",
    "    return examples, eval_examples, word_counts, char_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 130319 examples\r"
     ]
    }
   ],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "answers_text = []\n",
    "answers_starti = []\n",
    "impossibility = []\n",
    "topics = []\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "char_counts, word_counts = Counter(), Counter()\n",
    "\n",
    "train_examples, eval_examples, word_counts, char_counts = process_data(train, word_counts, char_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO: \n",
    " - think of a better method of keeping track than `total`\n",
    " - define the custom pytorch data structure\n",
    " - add word/vector embeddings\n",
    " - implement a basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Create the different models that will be used as a part of the baseline training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This section will contain all code used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
